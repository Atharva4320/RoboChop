{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-09-04T04:41:00.884510Z","iopub.status.busy":"2022-09-04T04:41:00.883875Z","iopub.status.idle":"2022-09-04T04:41:00.889698Z","shell.execute_reply":"2022-09-04T04:41:00.888553Z","shell.execute_reply.started":"2022-09-04T04:41:00.884474Z"}},"source":["***\n","***\n","<div style=\"background-color:#B0C4DE;color:Navy;width:100%;display:inline-block;text-align:center;\">\n","    <b><h1>&nbsp Fruit360 - PyTorch - CNN </h1></b>\n","</div>\n","\n","***\n","***\n","\n","Fruits360 Dataset consists of 90,483 fruit images (Training: 67692 - Test: 22688). All images have white background. Their size is 100x100. There are 131 fruit classes.\n","\n","I will use Convolutional Neural Networks in PyTorch. PyTorch will give us more control over the network (with extra workload).\n","\n","Since they have white background with clear fruit images, recognition will be easy with the correct architecture. I know images having no real world backgrounds will make it unsuccessfull in real world images.\n","\n","At first I have tried Adabound as optimizer, since starting with Adam and shifting to SGD + Momentum generaly shows good results. (Source: [Improving Generalization Performance by Switching from Adam to SGD](https://arxiv.org/abs/1712.07628)). Yet I see Adam is more succesfull in this dataset. So I choose Adam.\n","\n","Without Learning Rate Scheduler and Early Stopping, it achieves best accuracy approximately in 70th epoch. With some little changes, it now achieves best accuracy in between 20th-30th epoch.\n","\n","I have deleted most of the data augmentation code, since the model is better without them."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div style=\"background-color:#B0C4DE;color:Navy;width:100%;display:inline-block;\">\n","    <b><h2>&nbsp Index</h2></b>\n","</div>\n","\n","1. [Import Libraries](#1)\n","2. [Data Loaders](#2)\n","3. [Exploration](#3)\n","4. [Learning Rate Scheduler & EarlyStopping](#4)\n","5. [CNN Model](#5)\n","6. [Instantiate Classes](#6)\n","7. [Training](#7)\n","8. [Analyse Results](#8)\n","9. [Summary](#9)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div style=\"background-color:#B0C4DE;color:Navy;width:100%;display:inline-block;\">\n","    <b><h2>&nbsp <a id = \"1\">1 - Import Libraries </a></h2></b>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T16:37:00.409024Z","iopub.status.busy":"2022-09-04T16:37:00.407970Z","iopub.status.idle":"2022-09-04T16:37:14.223426Z","shell.execute_reply":"2022-09-04T16:37:14.222398Z","shell.execute_reply.started":"2022-09-04T16:37:00.408927Z"},"trusted":true},"outputs":[],"source":["# !pip install adabound\n","!pip install torchsummary \n","import numpy as np\n","import os, os.path\n","import math\n","from torchsummary import summary\n","from datetime import datetime\n","# from adabound import AdaBound\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","import matplotlib.image as mpimg"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div style=\"background-color:#B0C4DE;color:Navy;width:100%;display:inline-block;\">\n","    <b><h2>&nbsp <a id = \"2\">2 - Data Loaders </a></h2></b>\n","</div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T16:37:23.093894Z","iopub.status.busy":"2022-09-04T16:37:23.093024Z","iopub.status.idle":"2022-09-04T16:37:43.977555Z","shell.execute_reply":"2022-09-04T16:37:43.976516Z","shell.execute_reply.started":"2022-09-04T16:37:23.093856Z"},"trusted":true},"outputs":[],"source":["num_epochs = 100\n","batch_size = 512\n","image_height = 100\n","image_width = 100\n","num_channels = 3\n","\n","# train_dir = \"/home/alison/Desktop/Fruit360/fruits-360_dataset/fruits-360/Training\"\n","# test_dir = \"/home/alison/Desktop/Fruit360/fruits-360_dataset/fruits-360/Test\"\n","\n","train_dir = \"/home/alison/Desktop/ClassificationCutFruit/Training\"\n","test_dir = \"/home/alison/Desktop/ClassificationCutFruit/Test\"\n","\n","transforms_data = transforms.Compose([transforms.ToTensor(),\n","                                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","                                       transforms.Resize((image_height, image_width)),\n","                                       ])\n","\n","train_dataset = ImageFolder(train_dir, transform=transforms_data)\n","test_dataset = ImageFolder(test_dir, transform=transforms_data)\n","num_classes = len(train_dataset.classes)\n","\n","dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n","dataloader_test = DataLoader(test_dataset, batch_size=batch_size * 2, shuffle=False, pin_memory=True)\n","\n","print('Train Dataset Size: ', len(train_dataset))\n","print('Test Dataset Size: ', len(test_dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T16:37:50.067441Z","iopub.status.busy":"2022-09-04T16:37:50.064032Z","iopub.status.idle":"2022-09-04T16:37:50.147444Z","shell.execute_reply":"2022-09-04T16:37:50.146406Z","shell.execute_reply.started":"2022-09-04T16:37:50.067391Z"},"trusted":true},"outputs":[],"source":["CUDA = torch.cuda.is_available()\n","if CUDA:\n","    print(f\"Using GPU\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div style=\"background-color:#B0C4DE;color:Navy;width:100%;display:inline-block;\">\n","    <b><h2>&nbsp <a id = \"3\">3 - Exploration </a></h2></b>\n","</div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**First, we will show some fruits:**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T16:39:00.800266Z","iopub.status.busy":"2022-09-04T16:39:00.799860Z","iopub.status.idle":"2022-09-04T16:39:01.150754Z","shell.execute_reply":"2022-09-04T16:39:01.149771Z","shell.execute_reply.started":"2022-09-04T16:39:00.800232Z"},"trusted":true},"outputs":[],"source":["# fig, ax = plt.subplots(1,4,figsize=(12, 9), dpi=120)\n","# plt.setp(ax, xticks=[], yticks=[])\n","\n","# ax[0].imshow(mpimg.imread(train_dir+'/Apple Braeburn/0_100.jpg'))\n","# ax[1].imshow(mpimg.imread(train_dir+'/Banana/0_100.jpg'))\n","# ax[2].imshow(mpimg.imread(train_dir+'/Avocado ripe/0_100.jpg'))\n","# ax[3].imshow(mpimg.imread(train_dir+'/Cherry 2/105_100.jpg'))\n","\n","# plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Then, we will look into fruit category counts:**\n","\n","In the below plots we see that:\n","* The dataset is balanced, every category has similar counts with some exceptions\n","* Testing Data has the same counts as Training Data\n","\n","This is good for training. Otherwise, our model would have difficulty learning small-sized fruit categories."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T16:43:56.926693Z","iopub.status.busy":"2022-09-04T16:43:56.925991Z","iopub.status.idle":"2022-09-04T16:44:00.672515Z","shell.execute_reply":"2022-09-04T16:44:00.671665Z","shell.execute_reply.started":"2022-09-04T16:43:56.926654Z"},"trusted":true},"outputs":[],"source":["def plot_category_counts(path,xlabel,ylabel,title):\n","    categories = []\n","    counts = []\n","    for dir in os.listdir(path):\n","        categories.append(dir)\n","        counts.append(len(os.listdir(train_dir+\"/\"+ dir)))\n","    \n","    plt.rcParams[\"figure.figsize\"] = (40,20)\n","    index = np.arange(len(categories))\n","    plt.bar(index, counts)\n","    plt.xlabel(xlabel, fontsize=20)\n","    plt.ylabel(ylabel, fontsize=20)\n","    plt.xticks(index, categories, fontsize=15, rotation=90)\n","    plt.title(title, fontsize=30)\n","    plt.show()\n","\n","plot_category_counts(train_dir+\"/\",'Fruit Categories','Category Counts','Fruit Categories Training Distribution')\n","plot_category_counts(test_dir+\"/\",'Fruit Categories','Category Counts','Fruit Categories Testing Distribution')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div style=\"background-color:#B0C4DE;color:Navy;width:100%;display:inline-block;\">\n","    <b><h2>&nbsp <a id = \"4\">4 - Learning Rate Scheduler & EarlyStopping </a></h2></b>\n","</div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["To have more control in the future problems, I choose creating a class instead of using ReduceLROnPlateau directly. And there is an EarlyStopping class with minimum requirements."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T20:40:19.575769Z","iopub.status.busy":"2022-09-04T20:40:19.575140Z","iopub.status.idle":"2022-09-04T20:40:19.587190Z","shell.execute_reply":"2022-09-04T20:40:19.586089Z","shell.execute_reply.started":"2022-09-04T20:40:19.575726Z"},"trusted":true},"outputs":[],"source":["class LRScheduler():\n","    def __init__(self, optimizer, patience=5, min_lr=1e-7, factor=0.5):\n","        self.optimizer = optimizer\n","        self.patience = patience\n","        self.min_lr = min_lr\n","        self.factor = factor\n","        self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer,mode='min',patience=self.patience,factor=self.factor,min_lr=self.min_lr,verbose=True)\n","    def __call__(self, val_loss):\n","        self.lr_scheduler.step(val_loss)\n","\n","class EarlyStopping():\n","    def __init__(self, patience=5, min_delta=0,save_best=False):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.counter = 0\n","        self.best_loss = None\n","        self.early_stop = False\n","        self.save_best=save_best\n","    def __call__(self, val_loss):\n","        if self.best_loss == None:\n","            self.best_loss = val_loss\n","            if self.save_best:\n","                self.save_best_model()\n","        elif self.best_loss - val_loss > self.min_delta:\n","            self.best_loss = val_loss\n","            self.counter = 0\n","            if self.save_best:\n","                self.save_best_model()\n","        elif self.best_loss - val_loss < self.min_delta:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","    def save_best_model(self):\n","        print(\">>> Saving the current model with the best loss value...\")\n","        print(\"-\"*100)\n","        torch.save(model.state_dict(), 'best_loss_model_finetuned.pth')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div style=\"background-color:#B0C4DE;color:Navy;width:100%;display:inline-block;\">\n","    <b><h2>&nbsp <a id = \"5\">5 - CNN Model </a></h2></b>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T20:40:23.798762Z","iopub.status.busy":"2022-09-04T20:40:23.797627Z","iopub.status.idle":"2022-09-04T20:40:23.810824Z","shell.execute_reply":"2022-09-04T20:40:23.809766Z","shell.execute_reply.started":"2022-09-04T20:40:23.798693Z"},"trusted":true},"outputs":[],"source":["class Fruits_CNN(nn.Module):\n","    def __init__(self):\n","        super(Fruits_CNN, self).__init__()\n","        self.relu = nn.ReLU()\n","\n","        self.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=16, kernel_size=5, stride=1, padding='same')\n","        self.batchnorm1 = nn.BatchNorm2d(16)\n","        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        # 16*50*50\n","\n","        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=1, padding='same')\n","        self.batchnorm2 = nn.BatchNorm2d(32)\n","        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        # 32*25*25\n","\n","        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n","        self.batchnorm3 = nn.BatchNorm2d(64)\n","        self.maxpool3 = nn.MaxPool2d(kernel_size=5, stride=5)\n","        # 64*5*5 = 1600 >> it is in_features value for the self.linear1\n","\n","        self.flatten1 = nn.Flatten()\n","\n","        self.linear1 = nn.Linear(in_features=1600, out_features=512)\n","        self.dropout1 = nn.Dropout(p=0.25)\n","        self.linear2 = nn.Linear(512, num_classes)\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.batchnorm1(out)\n","        out = self.relu(out)\n","        out = self.maxpool1(out)\n","\n","        out = self.conv2(out)\n","        out = self.batchnorm2(out)\n","        out = self.relu(out)\n","        out = self.maxpool2(out)\n","\n","        out = self.conv3(out)\n","        out = self.batchnorm3(out)\n","        out = self.relu(out)\n","        out = self.maxpool3(out)\n","\n","        out = self.flatten1(out)\n","        out = self.linear1(out)\n","        out = self.relu(out)\n","        out = self.dropout1(out)\n","\n","        out = self.linear2(out)\n","\n","        return out"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div style=\"background-color:#B0C4DE;color:Navy;width:100%;display:inline-block;\">\n","    <b><h2>&nbsp <a id = \"6\">6 - Instantiate Classes </a></h2></b>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T20:40:27.320955Z","iopub.status.busy":"2022-09-04T20:40:27.320344Z","iopub.status.idle":"2022-09-04T20:40:27.347265Z","shell.execute_reply":"2022-09-04T20:40:27.346159Z","shell.execute_reply.started":"2022-09-04T20:40:27.320917Z"},"trusted":true},"outputs":[],"source":["model = Fruits_CNN()\n","# model.load_state_dict(torch.load('best_loss_model.pth'))\n","\n","if CUDA:\n","    model = model.cuda()\n","\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","lr_scheduler = LRScheduler(optimizer= optimizer,patience=5,min_lr=1e-7, factor=0.5)\n","early_stopping = EarlyStopping(patience=15, min_delta=0, save_best=True)\n","\n","print(summary(model, (num_channels, image_height,image_width),batch_size))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div style=\"background-color:#B0C4DE;color:Navy;width:100%;display:inline-block;\">\n","    <b><h2>&nbsp <a id = \"7\">7 - Training </a></h2></b>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T20:40:30.274019Z","iopub.status.busy":"2022-09-04T20:40:30.272186Z","iopub.status.idle":"2022-09-04T21:33:22.642796Z","shell.execute_reply":"2022-09-04T21:33:22.641702Z","shell.execute_reply.started":"2022-09-04T20:40:30.273970Z"},"trusted":true},"outputs":[],"source":["train_loss = []\n","test_loss = []\n","train_accuracy = []\n","test_accuracy = []\n","\n","print('Start Training')\n","print('*'*100)\n","\n","for epoch in range(num_epochs):\n","    start_time = datetime.now()\n","\n","    # TRAINING\n","    correct = 0\n","    iterations = 0\n","    iter_loss = 0.0\n","\n","    model.train()\n","    for i, (inputs, labels) in enumerate(dataloader_train):\n","        if CUDA:\n","            inputs = inputs.cuda()\n","            labels = labels.cuda()\n","\n","        outputs = model(inputs)\n","        loss = loss_fn(outputs, labels)\n","        iter_loss += loss.item()\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        _, predicted = torch.max(outputs, 1)\n","        correct += (predicted == labels).sum()\n","        iterations += 1\n","\n","    train_loss.append(iter_loss / iterations)\n","    train_accuracy.append(100 * correct / len(train_dataset))\n","\n","    # TESTING\n","    loss_testing = 0.0\n","    correct = 0\n","    iterations = 0\n","\n","    model.eval()\n","\n","    for i, (inputs, labels) in enumerate(dataloader_test):\n","        if CUDA:\n","            inputs = inputs.cuda()\n","            labels = labels.cuda()\n","\n","        outputs = model(inputs)\n","        loss = loss_fn(outputs, labels)\n","        loss_testing += loss.item()\n","        _, predicted = torch.max(outputs, 1)\n","        correct += (predicted == labels).sum()\n","\n","        iterations += 1\n","\n","    test_loss.append(loss_testing / iterations)\n","    test_accuracy.append(100 * correct / len(test_dataset))\n","\n","    print('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}, Testing Loss: {:.3f}, Testing Acc: {:.3f}'\n","          .format(epoch + 1, num_epochs, train_loss[-1], train_accuracy[-1], test_loss[-1], test_accuracy[-1]))\n","\n","    end_time = datetime.now()\n","    epoch_time = (end_time - start_time).total_seconds()\n","    print(\"-\"*100)\n","    print('Epoch Time : ', math.floor(epoch_time // 60), ':', math.floor(epoch_time % 60))\n","    print(\"-\"*100)\n","\n","    lr_scheduler(test_loss[-1])\n","    early_stopping(test_loss[-1])\n","    if early_stopping.early_stop:\n","        print('*** Early stopping ***')\n","        break\n","\n","print(\"*** Training Completed ***\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div style=\"background-color:#B0C4DE;color:Navy;width:100%;display:inline-block;\">\n","    <b><h2>&nbsp <a id = \"8\">8 - Analysing Results </a></h2></b>\n","</div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["I will not use confusion matrix, since:\n","* To show 131 classes in confusion matrix is hard\n","* Model is successfull enough\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","\n","model = Fruits_CNN().cuda()\n","model.load_state_dict(torch.load('best_loss_model_finetuned.pth'))\n","model.eval()\n","\n","y_test = []\n","y_pred = []\n","\n","for i, (inputs, labels) in enumerate(dataloader_test):\n","    if CUDA:\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","\n","        print(\"inputs: \", inputs.size())\n","\n","    outputs = model(inputs)\n","    _, predicted = torch.max(outputs, 1)\n","    y_test.append(labels.cpu().detach().numpy())\n","    y_pred.append(predicted.cpu().numpy())\n","\n","y_test = y_test[0]\n","y_pred = y_pred[0]\n","\n","conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n","\n","fig, ax = plt.subplots(figsize=(7.5, 7.5))\n","ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n","for i in range(conf_matrix.shape[0]):\n","    for j in range(conf_matrix.shape[1]):\n","        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n"," \n","plt.xlabel('Predictions', fontsize=18)\n","plt.ylabel('Actuals', fontsize=18)\n","plt.title('Confusion Matrix', fontsize=18)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import cv2\n","from scipy.ndimage import zoom\n","# -------- testing on real-world data ----------\n","label_dict = {0: 'Apple',\n","              1: 'Carrot',\n","              2: 'Cucumber',\n","              3: 'Lime',\n","              4: 'Orange',\n","              5: 'Pineapple'}\n","\n","images = cv2.imread(\"/home/alison/Desktop/Atharva/merged_image_5.jpg\")# import image to process\n","target_size = (100, 100, 3)\n","scale_factors = (np.array(target_size) / np.array(images.shape)).tolist()\n","images = zoom(images, scale_factors)\n","plt.imshow(images)\n","plt.show()\n","\n","\n","# dataset = ImageFolder(\"/home/alison/Desktop/Atharva/\", transform=transforms_data)\n","# num_classes = len(dataset.classes)\n","# dataloader = DataLoader(dataset, batch_size=1, shuffle=False, pin_memory=True)\n","\n","model = Fruits_CNN().cuda()\n","model.load_state_dict(torch.load('best_loss_model_finetuned.pth'))\n","model.eval()\n","\n","images = np.reshape(images, (images.shape[2], images.shape[0], images.shape[1]))\n","print(\"image shape: \", images.shape)\n","\n","CUDA = torch.cuda.is_available()\n","# device = torch.device('cuda')\n","if CUDA:\n","    images = torch.unsqueeze(torch.from_numpy(images), dim=0)\n","    print(images.size())\n","    images = images.cuda()\n","    images = images.float()\n","\n","outputs = model(images)\n","_, predicted = torch.max(outputs, 1)\n","predicted = predicted.cpu().detach().numpy()[0]\n","\n","print(\"Predicted Number: \", predicted)\n","print(\"Predicted Class: \", label_dict[predicted])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["train_accuracy and test accuracy is like:\n","\n","> [tensor(76.8569, device='cuda:0'),tensor(99.1757, device='cuda:0'),tensor(99.7666, device='cuda:0'), ...., tensor(100., device='cuda:0')] \n","\n","I don't want to change the training code. We need to convert them to plain lists as below: "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T21:46:49.483340Z","iopub.status.busy":"2022-09-04T21:46:49.482623Z","iopub.status.idle":"2022-09-04T21:46:49.490484Z","shell.execute_reply":"2022-09-04T21:46:49.489394Z","shell.execute_reply.started":"2022-09-04T21:46:49.483303Z"},"trusted":true},"outputs":[],"source":["def list_of_tensors_to_list(tensor_list):\n","    accuracy = []\n","    for tensor_item in tensor_list:\n","        accuracy.append(tensor_item.item())\n","    return accuracy\n","\n","train_accuracy = list_of_tensors_to_list(train_accuracy)\n","test_accuracy = list_of_tensors_to_list(test_accuracy)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Below are the loss and accuracy plots. I have changed ylim values for smoother lines. Otherwise pyplot uses smaller interval for ylim which makes the plot more spiky."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T21:46:54.766507Z","iopub.status.busy":"2022-09-04T21:46:54.765841Z","iopub.status.idle":"2022-09-04T21:46:55.069775Z","shell.execute_reply":"2022-09-04T21:46:55.068777Z","shell.execute_reply.started":"2022-09-04T21:46:54.766472Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(1,2,figsize=(8,4), dpi=120)\n","\n","# Loss\n","ax[0].plot(train_loss, label='Training Loss')\n","ax[0].plot(test_loss, label='Testing Loss')\n","# ax[0].axis(ymin=-0.10, ymax=10)\n","ax[0].set_title('Loss Plot')\n","ax[0].legend()\n","\n","# Accuracy\n","ax[1].plot(train_accuracy, label='Training Accuracy')\n","ax[1].plot(test_accuracy, label='Testing Accuracy')\n","# ax[1].axis(ymin=0, ymax=101)\n","ax[1].set_title('Accuracy Plot')\n","ax[1].legend()\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<div style=\"background-color:#B0C4DE;color:Navy;width:100%;display:inline-block;\">\n","    <b><h2>&nbsp <a id = \"9\">9 - Summary </a></h2></b>\n","</div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The model has approximately 99.30% test accuracy for a simple model. This is the power of CNN, not mine. \n","My next notebook will be about ResNet."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
